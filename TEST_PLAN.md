# Project COLDSNAP - End-to-End Test Plan

This document outlines the test cases for validating the complete integration of PROJECT COLDSNAP.

## 1. Environment and Server Startup Tests

* **Test Case 1.1: Python Server Startup (No Model)**
    * **Description:** Verify the Python inference server can start, even without a valid model, and report the correct error.
    * **Steps:**
        1. Activate Python virtual environment.
        2. Run `python openvino_inference_server.py --model-path ./non_existent_model_dir --device NPU`.
    * **Expected Outcome:** Server starts, prints "Error: Model path './non_existent_model_dir' not found or is not a valid directory.", and indicates LLMPipeline not initialized. Server listens on port 5001.

* **Test Case 1.2: Python Server Startup (With Model - Future)**
    * **Description:** Verify the Python inference server can start successfully with a correctly converted model.
    * **Steps:**
        1. Ensure `Phi-3-mini-4k-instruct-int4-npu` directory exists and contains the model.
        2. Activate Python virtual environment.
        3. Run `python openvino_inference_server.py --model-path ./Phi-3-mini-4k-instruct-int4-npu --device NPU`.
    * **Expected Outcome:** Server starts, prints "LLMPipeline initialized successfully for device: NPU", and listens on port 5001 without errors.

## 2. Node.js Application Integration Tests

* **Test Case 2.1: Model Option Display**
    * **Description:** Verify "Local OpenVINO (NPU/CPU)" appears as a selectable model in the `autocoder` UI.
    * **Steps:**
        1. Launch `autocoder` (`node index.js`).
        2. Navigate to the model selection interface.
    * **Expected Outcome:** "Local OpenVINO (NPU/CPU)" is listed as a model option.

* **Test Case 2.2: Local Model Selection (Server Down/No Model)**
    * **Description:** Test `autocoder`'s handling when the local OpenVINO server is unreachable or not initialized.
    * **Steps:**
        1. Ensure the Python server from Test Case 1.1 is running (or not running at all).
        2. In `autocoder`, select "Local OpenVINO (NPU/CPU)".
        3. Provide a prompt and attempt to generate code.
    * **Expected Outcome:** `autocoder` displays a "FATAL_ERROR: Could not connect to the local OpenVINO inference server..." or "LLMPipeline not initialized" message.

* **Test Case 2.3: Local Model Inference (Future)**
    * **Description:** Validate end-to-end inference using the local OpenVINO server.
    * **Steps:**
        1. Ensure the Python server from Test Case 1.2 is running with the model loaded.
        2. In `autocoder`, select "Local OpenVINO (NPU/CPU)".
        3. Provide a prompt (e.g., "Write a Python function to calculate factorial.").
        4. Attempt to generate code.
    * **Expected Outcome:** Code is generated by the local OpenVINO model and displayed in the `autocoder` UI. The server logs show the request and successful generation.

## 3. Fallback and Error Handling Tests (Future)

* **Test Case 3.1: Cloud API Fallback (Not directly implemented as fallback but distinct selection)**
    * **Description:** Verify existing cloud API model selections still function correctly.
    * **Steps:**
        1. Select "OpenAI GPT-4o" or "Gemini 1.5 Pro".
        2. Provide a prompt and generate code.
    * **Expected Outcome:** Code is generated by the selected cloud API.

---
This plan covers critical functionalities. Additional tests may be added for specific use cases or edge conditions.
